{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/dku_mse1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py\n",
    "import pandas as pd\n",
    "import params as PARAMS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ast\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv\n",
    "\n",
    "# Polarity 사전 로딩\n",
    "POS_LEX = set(opinion_lexicon.positive())\n",
    "NEG_LEX = set(opinion_lexicon.negative())\n",
    "\n",
    "\n",
    "\n",
    "def normalize_added_features(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df 안에서 *_polarity, *_mrc_conc, *_mrc_fam, *_local_idf, *_tf_score\n",
    "    열들만 골라서 MinMax 정규화(0~1).\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # 정규화 대상 컬럼만 찾기\n",
    "    target_suffixes = [\"_polarity\", \"_mrc_conc\", \"_mrc_fam\", \"_local_idf\", \"_tf_score\"]\n",
    "    columns_to_scale = [\n",
    "        col for col in df.columns\n",
    "        if any(col.endswith(suf) for suf in target_suffixes)\n",
    "    ]\n",
    "\n",
    "    if not columns_to_scale:\n",
    "        print(\"[INFO] No columns found for normalization.\")\n",
    "        return df\n",
    "\n",
    "    # 실제로 fit_transform 적용\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_mrc_data(mrc_file=\"mrc_database.csv\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    mrc_data.csv 예시:\n",
    "    word,concreteness,familiarity\n",
    "    apple,620,500\n",
    "    home,530,700\n",
    "    ...\n",
    "    \"\"\"\n",
    "    mrc_dict = {}\n",
    "    with open(mrc_file, \"r\") as f:      #, encoding=\"utf-8\"\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            w = row[\"Word\"].lower().strip()\n",
    "            conc = float(row[\"Concreteness\"])\n",
    "            fam  = float(row[\"Familiarity\"])\n",
    "            mrc_dict[w] = {\"conc\": conc, \"fam\": fam}\n",
    "\n",
    "        \n",
    "    return mrc_dict\n",
    "\n",
    "\n",
    "MRC_DICT = load_mrc_data()\n",
    "\n",
    "\n",
    "\n",
    "def analyze_sentence(sentence, idf_scores, mrc_dict):\n",
    "    \"\"\"\n",
    "    sentence: 단일 문장(str)\n",
    "    idf_scores: {'apple': 5.2, ...} (이미 util.py에서 구함)\n",
    "    mrc_dict:   {'apple': {'conc':620, 'fam':500}, ...}\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    word_count = len(words)\n",
    "    \n",
    "    # 1) Polarity: (pos_count - neg_count)\n",
    "    pos_count = sum(1 for w in words if w in POS_LEX)\n",
    "    neg_count = sum(1 for w in words if w in NEG_LEX)\n",
    "    polarity = pos_count - neg_count\n",
    "\n",
    "    # 2) MRC (concreteness, familiarity) 평균\n",
    "    conc_sum = 0.0\n",
    "    fam_sum  = 0.0\n",
    "    mrc_hits = 0\n",
    "    for w in words:\n",
    "        if w in mrc_dict:\n",
    "            conc_sum += mrc_dict[w][\"conc\"]\n",
    "            fam_sum  += mrc_dict[w][\"fam\"]\n",
    "            mrc_hits += 1\n",
    "    avg_conc = conc_sum / mrc_hits if mrc_hits else 0.0\n",
    "    avg_fam  = fam_sum  / mrc_hits if mrc_hits else 0.0\n",
    "\n",
    "    # 3) IDF: 단어별 IDF 합의 평균\n",
    "    if word_count > 0:\n",
    "        idf_sum = sum(idf_scores.get(w,1.0) for w in words)\n",
    "        avg_idf_local = idf_sum / word_count\n",
    "    else:\n",
    "        avg_idf_local = 0.0\n",
    "\n",
    "    # 4) TF: 아주 간단히 \"word_count\"를 TF로 볼 수도 있고,\n",
    "    #        혹은 \"단어별 빈도 / word_count\" 평균을 쓸 수도 있음\n",
    "    tf_score = float(word_count)  # 일단은 문장 길이를 TF로 취급\n",
    "\n",
    "    return {\n",
    "        \"polarity\": polarity,\n",
    "        \"avg_conc\": avg_conc,\n",
    "        \"avg_fam\":  avg_fam,\n",
    "        \"local_idf\": avg_idf_local,\n",
    "        \"tf_score\": tf_score\n",
    "    }\n",
    "\n",
    "def read_data():\n",
    "    def compute_idf(df):\n",
    "        def preprocess_text(text):\n",
    "            \"\"\"Remove numbers and keep only words in a sentence.\"\"\"\n",
    "            return ' '.join([word for word in text.split() if not any(char.isdigit() for char in word)])\n",
    "\n",
    "        corpus = []\n",
    "        \"\"\"Compute IDF scores for words across all text fields in the dataset.\"\"\"\n",
    "        for index, row in df.iterrows():\n",
    "            for category in PARAMS.FEATURES:\n",
    "                if category == \"Patient_ID\" or PARAMS.FULL_FEATURES[category] == 'int32':\n",
    "                    continue\n",
    "                sentences = row[category]\n",
    "                if isinstance(sentences, list) and sentences:\n",
    "                    # Preprocess each sentence to remove numbers before adding to corpus\n",
    "                    cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "                    corpus.extend(cleaned_sentences)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(use_idf=True)\n",
    "        vectorizer.fit(corpus)  # Learn IDF values from the corpus\n",
    "        idf_scores = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "        \n",
    "        return idf_scores\n",
    "    \n",
    "\n",
    "    def compute_max_sentences(df):\n",
    "        \"\"\"Compute the maximum number of sentences per category across the entire dataset.\"\"\"\n",
    "        max_sentences_per_category = {\n",
    "            category: df[category].apply(lambda x: len(x) if isinstance(x, list) else 0).mean()\n",
    "            for category in PARAMS.FEATURES\n",
    "            if category != \"Patient_ID\" and PARAMS.FULL_FEATURES[category] != 'int32'\n",
    "        }\n",
    "        return max_sentences_per_category\n",
    "\n",
    "    def compute_max_token_length(df):\n",
    "        from transformers import DistilBertTokenizer\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        \"\"\"Compute the maximum token length per category using DistilBERT tokenization.\"\"\"\n",
    "        max_token_length_per_category = {\n",
    "            category: df[category].apply(lambda x: max((len(tokenizer.tokenize(sentence)) for sentence in x), default=0)\n",
    "            if isinstance(x, list) else 0).max()\n",
    "            for category in PARAMS.FEATURES\n",
    "            if category != \"Patient_ID\" and PARAMS.FULL_FEATURES[category] != 'int32'\n",
    "        }\n",
    "        return max_token_length_per_category\n",
    "\n",
    "    def extract_statistics(df, idf_scores):\n",
    "        \"\"\"\n",
    "        기존 코드 + Polarity/MRC/TF등 추가\n",
    "        \"\"\"\n",
    "        # 먼저 MRC_DICT를 사용하기 위해 전역 변수를 가져오거나, \n",
    "        # 필요시 함수 인자로 받은 뒤 MRC_DICT = load_mrc_data() 할 수도 있음\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            row_stats = {}\n",
    "            for category in PARAMS.FEATURES:\n",
    "                if category == \"Patient_ID\" or PARAMS.FULL_FEATURES[category] == 'int32':\n",
    "                    continue\n",
    "\n",
    "                sentences = row[category]\n",
    "\n",
    "                if isinstance(sentences, list) and sentences:\n",
    "                    total_sentences = len(sentences)\n",
    "                    total_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "                    total_chars = sum(len(sentence) for sentence in sentences)\n",
    "                    total_numbers = sum(len(re.findall(r'\\d+', sentence)) for sentence in sentences)\n",
    "\n",
    "                    avg_words = total_words / total_sentences\n",
    "                    avg_chars = total_chars / total_sentences\n",
    "\n",
    "                    # 기존 IDF-weighted sum\n",
    "                    idf_weighted_sum = sum(\n",
    "                        sum(idf_scores.get(word, 1) for word in sentence.split()) for sentence in sentences)\n",
    "                    avg_idf_weight = idf_weighted_sum / total_words if total_words > 0 else 0\n",
    "\n",
    "                    # ▶ 추가: Polarity/MRC/TF (문장단위)\n",
    "                    polarity_sum = 0.0\n",
    "                    conc_sum = 0.0\n",
    "                    fam_sum  = 0.0\n",
    "                    idf_loc_sum = 0.0\n",
    "                    tf_sum = 0.0\n",
    "\n",
    "                    for sent in sentences:\n",
    "                        feats = analyze_sentence(sent, idf_scores, MRC_DICT)\n",
    "                        polarity_sum += feats[\"polarity\"]\n",
    "                        conc_sum     += feats[\"avg_conc\"]\n",
    "                        fam_sum      += feats[\"avg_fam\"]\n",
    "                        idf_loc_sum  += feats[\"local_idf\"]\n",
    "                        tf_sum       += feats[\"tf_score\"]\n",
    "\n",
    "                    # 문장별로 계산된 값의 평균\n",
    "                    avg_polarity = polarity_sum / total_sentences\n",
    "                    avg_conc     = conc_sum     / total_sentences\n",
    "                    avg_fam      = fam_sum      / total_sentences\n",
    "                    avg_idf_local= idf_loc_sum  / total_sentences\n",
    "                    avg_tf       = tf_sum       / total_sentences\n",
    "\n",
    "                else:\n",
    "                    # 문장이 없을 경우 0 처리\n",
    "                    total_sentences = 0\n",
    "                    avg_words = 0\n",
    "                    avg_chars = 0\n",
    "                    total_numbers = 0\n",
    "                    avg_idf_weight = 0\n",
    "\n",
    "                    avg_polarity = 0\n",
    "                    avg_conc = 0\n",
    "                    avg_fam  = 0\n",
    "                    avg_idf_local = 0\n",
    "                    avg_tf = 0\n",
    "\n",
    "                # 기존 5개\n",
    "                row_stats[f'{category}_avg_words'] = avg_words\n",
    "                row_stats[f'{category}_total_sentences'] = total_sentences\n",
    "                row_stats[f'{category}_avg_chars'] = avg_chars\n",
    "                row_stats[f'{category}_total_numbers'] = total_numbers\n",
    "                row_stats[f'{category}_avg_idf_weight'] = avg_idf_weight\n",
    "\n",
    "                # 추가 5개\n",
    "                row_stats[f'{category}_polarity']    = avg_polarity\n",
    "                row_stats[f'{category}_mrc_conc']    = avg_conc\n",
    "                row_stats[f'{category}_mrc_fam']     = avg_fam\n",
    "                row_stats[f'{category}_local_idf']   = avg_idf_local\n",
    "                row_stats[f'{category}_tf_score']    = avg_tf\n",
    "\n",
    "            result.append(row_stats)\n",
    "\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "\n",
    "    # df = pd.read_csv(PARAMS.DATASET_PATH, encoding_errors=\"ignore\")\n",
    "    df_ad = pd.read_csv(\"./data/250120/ad_combined.csv\", encoding_errors=\"ignore\")\n",
    "    df_mci = pd.read_csv(\"./data/250120/mci_combined.csv\", encoding_errors=\"ignore\")\n",
    "    df_nc = pd.read_csv(\"./data/250120/nc_combined.csv\", encoding_errors=\"ignore\")\n",
    "    df_mix = pd.read_csv(\"./data/250120/added_format.csv\", encoding_errors=\"ignore\")\n",
    "    df_nc2 = df_mix[df_mix['Label'] == 'NC']\n",
    "    df_nc = pd.concat([df_nc, df_nc2], ignore_index=True)\n",
    "    df_mci2 = df_mix[df_mix['Label'] == 'MCI']\n",
    "    df_mci = pd.concat([df_mci, df_mci2], ignore_index=True)\n",
    "    df_ad2 = df_mix[df_mix['Label'] == 'Dementia']\n",
    "    df_ad = pd.concat([df_ad, df_ad2], ignore_index=True)\n",
    "    df_nc['Label'] = \"NC\"\n",
    "    df_mci['Label'] = \"MCI\"\n",
    "    df_ad['Label'] = \"AD\"\n",
    "    df = pd.concat([df_nc, df_mci, df_ad], ignore_index=True)\n",
    "    # Convert string representations back to lists\n",
    "    df = df[[\"Label\", \"Original_ID\"] + PARAMS.FEATURES]\n",
    "    for col in PARAMS.FEATURES:\n",
    "        if col in [\"Patient_ID\", \"Gender\", \"Age\", \"Edu\"]:\n",
    "            continue\n",
    "        df[col] = df[col].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else x)\n",
    "    df = df.applymap(lambda x: [] if isinstance(x, float) and pd.isna(x) else x)\n",
    "    df = df[\n",
    "        ~((df['Age'].apply(lambda x: isinstance(x, list) and len(x) == 0)) |\n",
    "          (df['Edu'].apply(lambda x: isinstance(x, list) and len(x) == 0)))\n",
    "    ]\n",
    "    df = df.astype({'Original_ID': 'str', 'Patient_ID': 'Int32', 'Gender': 'Int32', 'Age': 'Int32', 'Edu': 'Int32'})\n",
    "\n",
    "    # df.nlargest(5, 'Edu')\n",
    "    df = df[df['Edu'] <= 50]\n",
    "    df = df[df['Age'] >= 30]\n",
    "    df = df.dropna(subset=PARAMS.FEATURES)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    idf_scores = compute_idf(df)  # Compute IDF scores from the dataset\n",
    "    stats_df = extract_statistics(df, idf_scores)\n",
    "\n",
    "    # label_encoder = LabelEncoder()\n",
    "    # df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
    "\n",
    "    # df['Age'] = df['Age'].astype(\"int\").astype(\"str\")\n",
    "    # columns = [\"Label\"] + PARAMS.FEATURES\n",
    "    # df = df.astype(PARAMS.FULL_FEATURES)\n",
    "    # df = df[columns]\n",
    "    # max_lengths = df.applymap(lambda x: len(str(x))).max()\n",
    "    df = pd.concat([df, stats_df], axis=1)\n",
    "    df = normalize_added_features(df)\n",
    "    df['label_encoded'] = df['Label'].map({cls: i for i, cls in enumerate(PARAMS.CLASSES)})\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label_encoded'], random_state=42)\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Original_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Edu</th>\n",
       "      <th>Main Complaints</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Language</th>\n",
       "      <th>Orientation</th>\n",
       "      <th>...</th>\n",
       "      <th>Personality and Behavior_total_sentences</th>\n",
       "      <th>Personality and Behavior_avg_chars</th>\n",
       "      <th>Personality and Behavior_total_numbers</th>\n",
       "      <th>Personality and Behavior_avg_idf_weight</th>\n",
       "      <th>Personality and Behavior_polarity</th>\n",
       "      <th>Personality and Behavior_mrc_conc</th>\n",
       "      <th>Personality and Behavior_mrc_fam</th>\n",
       "      <th>Personality and Behavior_local_idf</th>\n",
       "      <th>Personality and Behavior_tf_score</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>MCI</td>\n",
       "      <td>CDN0235</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>4</td>\n",
       "      <td>[1. Experiences significant shortness of breat...</td>\n",
       "      <td>[1. Reports subjective memory decline, particu...</td>\n",
       "      <td>[1. Capable of normal level conversation., 2. ...</td>\n",
       "      <td>[1. Has not gotten lost., 2. Visits hospitals ...</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>61.857143</td>\n",
       "      <td>8</td>\n",
       "      <td>3.392891</td>\n",
       "      <td>0.691729</td>\n",
       "      <td>0.468216</td>\n",
       "      <td>0.612005</td>\n",
       "      <td>0.619468</td>\n",
       "      <td>0.200608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>NC</td>\n",
       "      <td>CDN0464</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>[1. Subjective memory decline, evidenced by oc...</td>\n",
       "      <td>[1. Can remember most conversations from the p...</td>\n",
       "      <td>[1. Capable of ordinary conversation., 2. Unde...</td>\n",
       "      <td>[1. Time orientation: Cannot remember the exac...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>4.043067</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.505982</td>\n",
       "      <td>0.657970</td>\n",
       "      <td>0.709787</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>MCI</td>\n",
       "      <td>6916716</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>[1. Complains of memory decline for 3 years., ...</td>\n",
       "      <td>[1. Complains of memory decline starting 3 yea...</td>\n",
       "      <td>[1. Lacks fluency and word-finding difficultie...</td>\n",
       "      <td>[1. Date orientation: Year, month, day correct...</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>37.285714</td>\n",
       "      <td>7</td>\n",
       "      <td>2.686432</td>\n",
       "      <td>0.781955</td>\n",
       "      <td>0.489338</td>\n",
       "      <td>0.556060</td>\n",
       "      <td>0.518045</td>\n",
       "      <td>0.124620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>MCI</td>\n",
       "      <td>11004211</td>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>[1. Referred for screening assessment., 2. Occ...</td>\n",
       "      <td>[1. MMSE score is 12., 2. Refused SBT test., 3...</td>\n",
       "      <td>[1. Partially able to communicate due to heari...</td>\n",
       "      <td>[1. Partially oriented to time., 2. Well-orien...</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>43.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>3.436905</td>\n",
       "      <td>0.760234</td>\n",
       "      <td>0.482341</td>\n",
       "      <td>0.630073</td>\n",
       "      <td>0.623775</td>\n",
       "      <td>0.146572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>AD</td>\n",
       "      <td>4341138</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>[1. Complains of dizziness followed by falling...</td>\n",
       "      <td>[1. Complains of memory decline, remembering o...</td>\n",
       "      <td>[1. No fluency issues., 2. Word finding diffic...</td>\n",
       "      <td>[1. Difficulty with the current year, month, a...</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>10</td>\n",
       "      <td>2.141909</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.687473</td>\n",
       "      <td>0.726128</td>\n",
       "      <td>0.425834</td>\n",
       "      <td>0.091489</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label Original_ID  Patient_ID  Gender  Age  Edu  \\\n",
       "1701   MCI     CDN0235         150       0   88    4   \n",
       "488     NC     CDN0464         399       0   75    0   \n",
       "1200   MCI     6916716         195       0   73    0   \n",
       "1396   MCI    11004211         392       0   99    0   \n",
       "1865    AD     4341138          58       0   85    0   \n",
       "\n",
       "                                        Main Complaints  \\\n",
       "1701  [1. Experiences significant shortness of breat...   \n",
       "488   [1. Subjective memory decline, evidenced by oc...   \n",
       "1200  [1. Complains of memory decline for 3 years., ...   \n",
       "1396  [1. Referred for screening assessment., 2. Occ...   \n",
       "1865  [1. Complains of dizziness followed by falling...   \n",
       "\n",
       "                                                 Memory  \\\n",
       "1701  [1. Reports subjective memory decline, particu...   \n",
       "488   [1. Can remember most conversations from the p...   \n",
       "1200  [1. Complains of memory decline starting 3 yea...   \n",
       "1396  [1. MMSE score is 12., 2. Refused SBT test., 3...   \n",
       "1865  [1. Complains of memory decline, remembering o...   \n",
       "\n",
       "                                               Language  \\\n",
       "1701  [1. Capable of normal level conversation., 2. ...   \n",
       "488   [1. Capable of ordinary conversation., 2. Unde...   \n",
       "1200  [1. Lacks fluency and word-finding difficultie...   \n",
       "1396  [1. Partially able to communicate due to heari...   \n",
       "1865  [1. No fluency issues., 2. Word finding diffic...   \n",
       "\n",
       "                                            Orientation  ...  \\\n",
       "1701  [1. Has not gotten lost., 2. Visits hospitals ...  ...   \n",
       "488   [1. Time orientation: Cannot remember the exac...  ...   \n",
       "1200  [1. Date orientation: Year, month, day correct...  ...   \n",
       "1396  [1. Partially oriented to time., 2. Well-orien...  ...   \n",
       "1865  [1. Difficulty with the current year, month, a...  ...   \n",
       "\n",
       "     Personality and Behavior_total_sentences  \\\n",
       "1701                                        7   \n",
       "488                                         3   \n",
       "1200                                        7   \n",
       "1396                                        9   \n",
       "1865                                       10   \n",
       "\n",
       "     Personality and Behavior_avg_chars  \\\n",
       "1701                          61.857143   \n",
       "488                           67.333333   \n",
       "1200                          37.285714   \n",
       "1396                          43.888889   \n",
       "1865                          25.400000   \n",
       "\n",
       "     Personality and Behavior_total_numbers  \\\n",
       "1701                                      8   \n",
       "488                                       3   \n",
       "1200                                      7   \n",
       "1396                                      9   \n",
       "1865                                     10   \n",
       "\n",
       "     Personality and Behavior_avg_idf_weight  \\\n",
       "1701                                3.392891   \n",
       "488                                 4.043067   \n",
       "1200                                2.686432   \n",
       "1396                                3.436905   \n",
       "1865                                2.141909   \n",
       "\n",
       "     Personality and Behavior_polarity  Personality and Behavior_mrc_conc  \\\n",
       "1701                          0.691729                           0.468216   \n",
       "488                           0.701754                           0.505982   \n",
       "1200                          0.781955                           0.489338   \n",
       "1396                          0.760234                           0.482341   \n",
       "1865                          0.810526                           0.687473   \n",
       "\n",
       "      Personality and Behavior_mrc_fam  Personality and Behavior_local_idf  \\\n",
       "1701                          0.612005                            0.619468   \n",
       "488                           0.657970                            0.709787   \n",
       "1200                          0.556060                            0.518045   \n",
       "1396                          0.630073                            0.623775   \n",
       "1865                          0.726128                            0.425834   \n",
       "\n",
       "      Personality and Behavior_tf_score  label_encoded  \n",
       "1701                           0.200608              1  \n",
       "488                            0.234043              0  \n",
       "1200                           0.124620              1  \n",
       "1396                           0.146572              1  \n",
       "1865                           0.091489              2  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = read_data()\n",
    "df = (train_df, val_df)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
